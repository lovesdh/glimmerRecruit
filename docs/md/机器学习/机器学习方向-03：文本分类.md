![](https://img1.imgtp.com/2024/09/10/081jz2kT.PNG)

# **机器学习**-03：文本分类任务

>`难度系数`：中等
>
>在学习了有关梯度下降的知识之后，可以开始尝试深度学习框架了！深度学习框架是用于构建、训练和部署深度学习模型的软件工具。它们提供了一系列的函数、类和工具，使得开发者可以方便地定义、优化和执行深度学习模型。常见的深度学习框架有tensorflow、pytorch、keras等，选择一种框架进行学习，在学习过程中记录下你的思考与体验。

## 前置知识

### NLP前置知识

在理解和应用 RNN 类模型进行文本分类之前，有几个重要的 NLP 前置知识需要了解：

> **文本分类：** 文本分类是一种自然语言处理（NLP）任务，旨在将给定的文本分配到预定义的类别或标签中。它是根据文本的内容和语义特征来判断文本所属类别的过程。在文本分类任务中，通常有一个已知的类别集合，每个类别代表一个特定的主题、情感或类别。模型的目标是根据文本的特征和上下文信息，将其正确地分类到相应的类别中。
>
> **文本预处理：** 包括文本清洗、分词、去除停用词等，以便将文本数据转化为可供模型处理的形式。
>
> **词嵌入**（Word Embeddings）：将文本中的单词映射为低维的实数向量表示，以捕捉单词之间的语义关系。
>
> **循环神经网络**（Recurrent Neural Networks，RNN）：一种能够处理序列数据的神经网络模型，通过在每个时间步骤上传递隐藏状态，可以捕捉到序列中的上下文信息。
>
> **长短时记忆网络**（Long Short-Term Memory，LSTM）：一种RNN的变体，通过引入门控机制，可以更好地处理长期依赖关系。
>
> **注意⭐️：在本次任务中，我们并不限定任何模型和实现方式。RNN和LSTM不是必须的，推荐使用，还未接触RNN的初学者同学也可以使用MLP模型（是的MLP也是可以用的，只是效果差点，但是我们强烈推荐初学者同学用MLP来一次实践），高阶的同学甚至可以直接上BERT模型**

### 文本处理前置知识

文本处理是指对文本数据进行**预处理和转换**的过程，以便于后续的文本分析、挖掘和建模。文本处理的目标是清洗、规范和转换原始文本数据，以提取有用的信息和特征，从而支持各种文本相关的任务，如文本分类、情感分析、机器翻译等。

下面是一些常见的文本处理步骤：

1. 文本清洗：去除文本中的特殊字符、标点符号和HTML标签等无关信息。处理大小写，可以将文本转换为小写形式，以避免同一个单词因大小写不同而被视为不同的词汇。

```python
import re
def clean_text(text):
    # 去除特殊字符和标点符号
    text = re.sub(r"[^a-zA-Z0-9]", " ", text)
    # 将文本转换为小写
    text = text.lower()
    # 去除多余的空格
    text = re.sub(r"\s+", " ", text)
    return text
# 示例文本
text = "Hello, this is an example text! It contains special characters and punctuation."
# 清洗文本
cleaned_text = clean_text(text)
print(cleaned_text)
```

在以上代码中，`clean_text`函数使用正则表达式去除了特殊字符和标点符号，将文本转换为小写，并去除了多余的空格。你可以根据需要进行修改和扩展，例如添加停用词移除、词干提取等其他文本清洗步骤。

2. 分词（Tokenization）：将文本分割成单词或子词的序列。

```Python
import re
def tokenize(text):
    # 将文本中的标点符号替换为空格
    text = re.sub(r'[^\w\s]', ' ', text)
    # 将文本按空格分割成单词列表
    tokens = text.split()
    return tokens
# 示例文本
text = "This is an example sentence for tokenization."
# 分词
tokens = tokenize(text)
print(tokens)
```

3. 去除停用词（Stop Words）：停用词是在文本中频繁出现但通常不携带重要信息的常见词汇，如介词、连词和冠词等。去除停用词可以减少文本的维度，并提高模型的效果。

```Python
# 停用词列表
stop_words = ["the", "is", "an", "of"]
# 示例文本
text = "This is an example sentence demonstrating the removal of stopwords."
# 分词
tokens = text.split()
# 去除停用词
filtered_tokens = [word for word in tokens if word.lower() not in stop_words]
# 打印结果
print("Original Text:")
print(text)
print()
print("Filtered Text:")
print(' '.join(filtered_tokens))
```

在上述代码中，首先定义了一个停用词列（`stop_words`），其中包含要去除的常见停用词。定义了一个示例文本（`text`）。接下来，使用`split()`方法将文本分割成单词，得到一个单词列表（`tokens`）。然后，使用列表推导式，遍历分词后的单词列表，并将不在停用词列表中的单词保留下来（使用`lower()`方法将单词转换为小写进行比较）。最后，使用`join()`方法将过滤后的单词列表重新组合成一个字符串，并打印结果。

**注意：** 上述代码均为示例，与本题无关，实际在完成本题过程中，需要根据具体任务和数据进行相应的调整和修改。

## 题目-情绪分类

* 本题旨在训练一个文本二分类模型，用以预测每个句子所表达的情绪是正面还是负面
* 可以使用 MLP 或者 RNN 类模型等，不强制使用任何模型或实现方法，自由发挥！
* 可使用深度学习框架，框架不限，推荐使用 tensorflow2 或 pytorch

## 举个栗子🌰

**下面是一个使用** **pytorch** **框架+ LSTM 模型的代码示例**

### **环境配置**

1. 安装 Anaconda,学习简单的 conda 命令
2. 推荐选择 tensorflow 或者 pytorch 框架，则创建相应环境，安装 tensorflow/pytorch，以及 GPU 配置（建议安装，需要电脑有独显）

### **数据处理**

- 获取数据集（招新群 683234808）或者从 kaggle 上下载 [ML2020spring - hw4 | Kaggle](https://www.kaggle.com/competitions/ml2020spring-hw4/overview)
- **在该数据集中只会用到 training_label.txt，并且请自行划分训练、验证、测试集**
- 训练集的每一行为一个数据，每个数据分别包含一个文本和一个 label（0表示消极情绪，1表示积极情绪）
- 拿到数据之后，需要对数据进行文本处理（可参考上文），再建立词表，以及做 word2id、label2id 的映射等：

```Python
def create_corpus(texts):
    """
    通过对训练文本做文本处理（参考上文）并统计分词结果，建立词表
    小Tips：建立词表的时候别忘了加一些特殊token，如：
        [PAD]：用于id序列的padding的token
        [UNK]：用于映射不存在于词表的词
    请自行实现
    """

def preprocess_text(text):
    """
    对数据进行处理，包括文本处理（参考上文）、做word2id映射、id序列的padding等
    padding：
        由于将不同的文本序列组成一个batch时，需要每个文本的长度相同
        所以需要对某些文本进行截断或填充，填充的word即为[PAD]
        如：将I love you填充至seq_len=5，即为I love you [PAD] [PAD]
    请自行实现
    """
    
def preprocess_label(label):
    """
    包括label2id的映射等
    请自行实现
    """
```

### **Dataset**

- 在pytorch中，需要建立`Dataset()`定义我们处理数据的方式，其中的`__getitem()__`即定义了每单个数据的处理方法，之后在训练时再用`DataLoader()`进行封装，即可组成一个 batch 的数据

```Python
class CustomDataset(torch.utils.data.Dataset):
    def __init__(self, texts, labels):
        self.texts = texts
        self.labels = labels

    def __getitem__(self, index):
        text = preprocess_text(self.texts[index])
        label = preprocess_label(self.labels[index])
        
        return text, label

    def __len__(self):
        return len(self.texts)
```

### **构造模型**

- 接下来就是如何搭建模型，这里使用 LSTM 做实例，大家也可以尽情尝试其他模型
- ❗**请一定保证你使用的模型是你会的，比如如果你用了 LSTM，那么我们则会在面试时抽查 LSTM 相关的知识**❗
- 文本任务都会涉及到 embedding，其本质是存储了一个向量矩阵，该矩阵`shape=(vocab_size, embedding_dim)`，在前向传播时，会将每个词都映射为一个 embedding_dim 维的向量

```Python
class LSTMModel(torch.nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout=0.1):
        super(LSTMModel, self).__init__()
        # embedding是将某个词对应的id映射到向量，然后用这个向量作为模型的输入，该向量的维度为embedding_dim
        self.embedding = torch.nn.Embedding(vocab_size, embedding_dim)
        # 这里用的LSTM模型，也可以尝试其他模型
        self.LSTM = torch.nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)
        self.classifier = torch.nn.Sequential(
            torch.nn.Dropout(dropout),
            torch.nn.Linear(hidden_dim, 2)
        )

    def forward(self, inputs):
        # 最开始的输入inputs.shape = (batch_size, seq_len)
        inputs = self.embedding(inputs)
        # 过了self.embdding之后，inputs.shape = (batch_size, seq_len, embedding_dim)
        x, _ = self.LSTM(inputs)
        # x.shape = (batch_size, seq_len, hidden_size)
        # 取用 LSTM 最后一个的 hidden state
        x = x[:, -1, :]
        x = self.classifier(x)

        return x
```

### **训练模型**

- 首先定义需要的组件，这里损失采用交叉熵损失，优化器采用 Adam

```Python
model = LSTMModel(
        vocab_size=30000,   # vocab_size为词表大小
        embedding_dim=300,
        hidden_dim=256,
        num_layers=1,
        dropout=0.1,
).cuda()    # 将模型移入GPU，只有用GPU时才需要.cuda()
# 定义损失函数和优化器
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
```

- 接下来就是如何读取数据，并组成 batch 的数据，需要用到上文的`CustomDataset()`

```Python
train_dataset = CustomDataset(texts_train, labels_train)
train_loader = torch.utils.data.DataLoader(dataset=train_dataset,
     batch_size=batch_size, shuffle=True)
```

- 然后就可以开始训练了

```Python
for epoch in range(num_epochs):
    for inputs, labels in train_loader:
        # 将tensor移入GPU
        inputs = inputs.cuda()
        labels = labels.cuda()
        
        optimizer.zero_grad()   # 将上一次迭代的梯度清凉
        outputs = model(inputs)   # 前向传播
        loss = criterion(outputs, labels)   # 计算损失
        loss.backward()    # 反向传播，获得梯度
        optimizer.step()    # 通过梯度，进行参数更新
        
# 训练完毕，保存模型
torch.save(model, "./model.pt")
```

### **评估模型**

- 评估模型，我们还需要划分出一个验证/测试集，并且同样建立`DataLoader()`，假设为`val_loader`

```Python
# 加载模型
model = torch.load("./model.pt")

for inputs, labels in val_loader:
    outputs = model(inputs)
    # 现在的outputs为一个batch的logits，并且shape=(batch_size, 2)
    # 请你通过这个outputs矩阵，结合你所挑选的模型评估指标（如准确率），对模型进行评估
    # 请自行实现
```

**注意**，**上述均只是一个示例，实际在完成本题过程中，需要根据具体任务和数据进行相应的调整和修改。**

## 思考

1. 你使用的是什么损失函数？请简单介绍这个损失函数，如果有概率论基础并了解最大似然的同学，请尝试推导出损失函数的数学形式。
2. 在文本分类任务中可能会面临过拟合问题，尤其是当训练数据较少时，可以采用哪些常见的防止过拟合的方法。
3. 反向传播是一种用于训练神经网络模型的算法，通过计算损失函数对模型参数的梯度，然后利用梯度下降法更新模型参数。请思考并简单推导一下**你所用的这个模型的**反向传播公式。
4. 在 LSTM 中，输入门、遗忘门和输出门是如何实现其功能的。

## 回答要求

1. 处理数据、训练和测试的代码，并大致解释你的代码；
2. 代码运行结果截图和训练过程中，损失和**评估指标**（**评估指标不限，如准确率**）的变化图像，并在最后使用你所划分的测试集和你选择的评估指标评估模型结果；
3. 必要的注释说明及良好的代码规范；
4. 实现思路和学到的知识点。

## 本题提交方式

> 收件邮箱：glimmer401@outlook.com  
>
> 主题格式：学号-姓名-考核-机器学习-03
>
> 主题示例：2024091202014-张三-考核-机器学习-03

> 出题人QQ：674940575
>
> 出题人邮箱：[674940575@qq.com](mailto:674940575@qq.com)




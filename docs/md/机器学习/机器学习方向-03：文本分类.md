![](https://img1.imgtp.com/2023/09/10/081jz2kT.PNG)

# **æœºå™¨å­¦ä¹ **-03ï¼šæ–‡æœ¬åˆ†ç±»ä»»åŠ¡

>`éš¾åº¦ç³»æ•°`ï¼šä¸­ç­‰
>
>åœ¨å­¦ä¹ äº†æœ‰å…³æ¢¯åº¦ä¸‹é™çš„çŸ¥è¯†ä¹‹åŽï¼Œå¯ä»¥å¼€å§‹å°è¯•æ·±åº¦å­¦ä¹ æ¡†æž¶äº†ï¼æ·±åº¦å­¦ä¹ æ¡†æž¶æ˜¯ç”¨äºŽæž„å»ºã€è®­ç»ƒå’Œéƒ¨ç½²æ·±åº¦å­¦ä¹ æ¨¡åž‹çš„è½¯ä»¶å·¥å…·ã€‚å®ƒä»¬æä¾›äº†ä¸€ç³»åˆ—çš„å‡½æ•°ã€ç±»å’Œå·¥å…·ï¼Œä½¿å¾—å¼€å‘è€…å¯ä»¥æ–¹ä¾¿åœ°å®šä¹‰ã€ä¼˜åŒ–å’Œæ‰§è¡Œæ·±åº¦å­¦ä¹ æ¨¡åž‹ã€‚å¸¸è§çš„æ·±åº¦å­¦ä¹ æ¡†æž¶æœ‰tensorflowã€pytorchã€kerasç­‰ï¼Œé€‰æ‹©ä¸€ç§æ¡†æž¶è¿›è¡Œå­¦ä¹ ï¼Œåœ¨å­¦ä¹ è¿‡ç¨‹ä¸­è®°å½•ä¸‹ä½ çš„æ€è€ƒä¸Žä½“éªŒã€‚

## å‰ç½®çŸ¥è¯†

### NLPå‰ç½®çŸ¥è¯†

åœ¨ç†è§£å’Œåº”ç”¨ RNN ç±»æ¨¡åž‹è¿›è¡Œæ–‡æœ¬åˆ†ç±»ä¹‹å‰ï¼Œæœ‰å‡ ä¸ªé‡è¦çš„ NLP å‰ç½®çŸ¥è¯†éœ€è¦äº†è§£ï¼š

> **æ–‡æœ¬åˆ†ç±»ï¼š** æ–‡æœ¬åˆ†ç±»æ˜¯ä¸€ç§è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡ï¼Œæ—¨åœ¨å°†ç»™å®šçš„æ–‡æœ¬åˆ†é…åˆ°é¢„å®šä¹‰çš„ç±»åˆ«æˆ–æ ‡ç­¾ä¸­ã€‚å®ƒæ˜¯æ ¹æ®æ–‡æœ¬çš„å†…å®¹å’Œè¯­ä¹‰ç‰¹å¾æ¥åˆ¤æ–­æ–‡æœ¬æ‰€å±žç±»åˆ«çš„è¿‡ç¨‹ã€‚åœ¨æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œé€šå¸¸æœ‰ä¸€ä¸ªå·²çŸ¥çš„ç±»åˆ«é›†åˆï¼Œæ¯ä¸ªç±»åˆ«ä»£è¡¨ä¸€ä¸ªç‰¹å®šçš„ä¸»é¢˜ã€æƒ…æ„Ÿæˆ–ç±»åˆ«ã€‚æ¨¡åž‹çš„ç›®æ ‡æ˜¯æ ¹æ®æ–‡æœ¬çš„ç‰¹å¾å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå°†å…¶æ­£ç¡®åœ°åˆ†ç±»åˆ°ç›¸åº”çš„ç±»åˆ«ä¸­ã€‚
>
> **æ–‡æœ¬é¢„å¤„ç†ï¼š** åŒ…æ‹¬æ–‡æœ¬æ¸…æ´—ã€åˆ†è¯ã€åŽ»é™¤åœç”¨è¯ç­‰ï¼Œä»¥ä¾¿å°†æ–‡æœ¬æ•°æ®è½¬åŒ–ä¸ºå¯ä¾›æ¨¡åž‹å¤„ç†çš„å½¢å¼ã€‚
>
> **è¯åµŒå…¥**ï¼ˆWord Embeddingsï¼‰ï¼šå°†æ–‡æœ¬ä¸­çš„å•è¯æ˜ å°„ä¸ºä½Žç»´çš„å®žæ•°å‘é‡è¡¨ç¤ºï¼Œä»¥æ•æ‰å•è¯ä¹‹é—´çš„è¯­ä¹‰å…³ç³»ã€‚
>
> **å¾ªçŽ¯ç¥žç»ç½‘ç»œ**ï¼ˆRecurrent Neural Networksï¼ŒRNNï¼‰ï¼šä¸€ç§èƒ½å¤Ÿå¤„ç†åºåˆ—æ•°æ®çš„ç¥žç»ç½‘ç»œæ¨¡åž‹ï¼Œé€šè¿‡åœ¨æ¯ä¸ªæ—¶é—´æ­¥éª¤ä¸Šä¼ é€’éšè—çŠ¶æ€ï¼Œå¯ä»¥æ•æ‰åˆ°åºåˆ—ä¸­çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚
>
> **é•¿çŸ­æ—¶è®°å¿†ç½‘ç»œ**ï¼ˆLong Short-Term Memoryï¼ŒLSTMï¼‰ï¼šä¸€ç§RNNçš„å˜ä½“ï¼Œé€šè¿‡å¼•å…¥é—¨æŽ§æœºåˆ¶ï¼Œå¯ä»¥æ›´å¥½åœ°å¤„ç†é•¿æœŸä¾èµ–å…³ç³»ã€‚
>
> **æ³¨æ„â­ï¸ï¼šåœ¨æœ¬æ¬¡ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬å¹¶ä¸é™å®šä»»ä½•æ¨¡åž‹å’Œå®žçŽ°æ–¹å¼ã€‚RNNå’ŒLSTMä¸æ˜¯å¿…é¡»çš„ï¼ŒæŽ¨èä½¿ç”¨ï¼Œè¿˜æœªæŽ¥è§¦RNNçš„åˆå­¦è€…åŒå­¦ä¹Ÿå¯ä»¥ä½¿ç”¨MLPæ¨¡åž‹ï¼ˆæ˜¯çš„MLPä¹Ÿæ˜¯å¯ä»¥ç”¨çš„ï¼Œåªæ˜¯æ•ˆæžœå·®ç‚¹ï¼Œä½†æ˜¯æˆ‘ä»¬å¼ºçƒˆæŽ¨èåˆå­¦è€…åŒå­¦ç”¨MLPæ¥ä¸€æ¬¡å®žè·µï¼‰ï¼Œé«˜é˜¶çš„åŒå­¦ç”šè‡³å¯ä»¥ç›´æŽ¥ä¸ŠBERTæ¨¡åž‹**

### æ–‡æœ¬å¤„ç†å‰ç½®çŸ¥è¯†

æ–‡æœ¬å¤„ç†æ˜¯æŒ‡å¯¹æ–‡æœ¬æ•°æ®è¿›è¡Œ**é¢„å¤„ç†å’Œè½¬æ¢**çš„è¿‡ç¨‹ï¼Œä»¥ä¾¿äºŽåŽç»­çš„æ–‡æœ¬åˆ†æžã€æŒ–æŽ˜å’Œå»ºæ¨¡ã€‚æ–‡æœ¬å¤„ç†çš„ç›®æ ‡æ˜¯æ¸…æ´—ã€è§„èŒƒå’Œè½¬æ¢åŽŸå§‹æ–‡æœ¬æ•°æ®ï¼Œä»¥æå–æœ‰ç”¨çš„ä¿¡æ¯å’Œç‰¹å¾ï¼Œä»Žè€Œæ”¯æŒå„ç§æ–‡æœ¬ç›¸å…³çš„ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬åˆ†ç±»ã€æƒ…æ„Ÿåˆ†æžã€æœºå™¨ç¿»è¯‘ç­‰ã€‚

ä¸‹é¢æ˜¯ä¸€äº›å¸¸è§çš„æ–‡æœ¬å¤„ç†æ­¥éª¤ï¼š

1. æ–‡æœ¬æ¸…æ´—ï¼šåŽ»é™¤æ–‡æœ¬ä¸­çš„ç‰¹æ®Šå­—ç¬¦ã€æ ‡ç‚¹ç¬¦å·å’ŒHTMLæ ‡ç­¾ç­‰æ— å…³ä¿¡æ¯ã€‚å¤„ç†å¤§å°å†™ï¼Œå¯ä»¥å°†æ–‡æœ¬è½¬æ¢ä¸ºå°å†™å½¢å¼ï¼Œä»¥é¿å…åŒä¸€ä¸ªå•è¯å› å¤§å°å†™ä¸åŒè€Œè¢«è§†ä¸ºä¸åŒçš„è¯æ±‡ã€‚

```python
import re
def clean_text(text):
    # åŽ»é™¤ç‰¹æ®Šå­—ç¬¦å’Œæ ‡ç‚¹ç¬¦å·
    text = re.sub(r"[^a-zA-Z0-9]", " ", text)
    # å°†æ–‡æœ¬è½¬æ¢ä¸ºå°å†™
    text = text.lower()
    # åŽ»é™¤å¤šä½™çš„ç©ºæ ¼
    text = re.sub(r"\s+", " ", text)
    return text
# ç¤ºä¾‹æ–‡æœ¬
text = "Hello, this is an example text! It contains special characters and punctuation."
# æ¸…æ´—æ–‡æœ¬
cleaned_text = clean_text(text)
print(cleaned_text)
```

åœ¨ä»¥ä¸Šä»£ç ä¸­ï¼Œ`clean_text`å‡½æ•°ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŽ»é™¤äº†ç‰¹æ®Šå­—ç¬¦å’Œæ ‡ç‚¹ç¬¦å·ï¼Œå°†æ–‡æœ¬è½¬æ¢ä¸ºå°å†™ï¼Œå¹¶åŽ»é™¤äº†å¤šä½™çš„ç©ºæ ¼ã€‚ä½ å¯ä»¥æ ¹æ®éœ€è¦è¿›è¡Œä¿®æ”¹å’Œæ‰©å±•ï¼Œä¾‹å¦‚æ·»åŠ åœç”¨è¯ç§»é™¤ã€è¯å¹²æå–ç­‰å…¶ä»–æ–‡æœ¬æ¸…æ´—æ­¥éª¤ã€‚

2. åˆ†è¯ï¼ˆTokenizationï¼‰ï¼šå°†æ–‡æœ¬åˆ†å‰²æˆå•è¯æˆ–å­è¯çš„åºåˆ—ã€‚

```Python
import re
def tokenize(text):
    # å°†æ–‡æœ¬ä¸­çš„æ ‡ç‚¹ç¬¦å·æ›¿æ¢ä¸ºç©ºæ ¼
    text = re.sub(r'[^\w\s]', ' ', text)
    # å°†æ–‡æœ¬æŒ‰ç©ºæ ¼åˆ†å‰²æˆå•è¯åˆ—è¡¨
    tokens = text.split()
    return tokens
# ç¤ºä¾‹æ–‡æœ¬
text = "This is an example sentence for tokenization."
# åˆ†è¯
tokens = tokenize(text)
print(tokens)
```

3. åŽ»é™¤åœç”¨è¯ï¼ˆStop Wordsï¼‰ï¼šåœç”¨è¯æ˜¯åœ¨æ–‡æœ¬ä¸­é¢‘ç¹å‡ºçŽ°ä½†é€šå¸¸ä¸æºå¸¦é‡è¦ä¿¡æ¯çš„å¸¸è§è¯æ±‡ï¼Œå¦‚ä»‹è¯ã€è¿žè¯å’Œå† è¯ç­‰ã€‚åŽ»é™¤åœç”¨è¯å¯ä»¥å‡å°‘æ–‡æœ¬çš„ç»´åº¦ï¼Œå¹¶æé«˜æ¨¡åž‹çš„æ•ˆæžœã€‚

```Python
# åœç”¨è¯åˆ—è¡¨
stop_words = ["the", "is", "an", "of"]
# ç¤ºä¾‹æ–‡æœ¬
text = "This is an example sentence demonstrating the removal of stopwords."
# åˆ†è¯
tokens = text.split()
# åŽ»é™¤åœç”¨è¯
filtered_tokens = [word for word in tokens if word.lower() not in stop_words]
# æ‰“å°ç»“æžœ
print("Original Text:")
print(text)
print()
print("Filtered Text:")
print(' '.join(filtered_tokens))
```

åœ¨ä¸Šè¿°ä»£ç ä¸­ï¼Œé¦–å…ˆå®šä¹‰äº†ä¸€ä¸ªåœç”¨è¯åˆ—ï¼ˆ`stop_words`ï¼‰ï¼Œå…¶ä¸­åŒ…å«è¦åŽ»é™¤çš„å¸¸è§åœç”¨è¯ã€‚å®šä¹‰äº†ä¸€ä¸ªç¤ºä¾‹æ–‡æœ¬ï¼ˆ`text`ï¼‰ã€‚æŽ¥ä¸‹æ¥ï¼Œä½¿ç”¨`split()`æ–¹æ³•å°†æ–‡æœ¬åˆ†å‰²æˆå•è¯ï¼Œå¾—åˆ°ä¸€ä¸ªå•è¯åˆ—è¡¨ï¼ˆ`tokens`ï¼‰ã€‚ç„¶åŽï¼Œä½¿ç”¨åˆ—è¡¨æŽ¨å¯¼å¼ï¼ŒéåŽ†åˆ†è¯åŽçš„å•è¯åˆ—è¡¨ï¼Œå¹¶å°†ä¸åœ¨åœç”¨è¯åˆ—è¡¨ä¸­çš„å•è¯ä¿ç•™ä¸‹æ¥ï¼ˆä½¿ç”¨`lower()`æ–¹æ³•å°†å•è¯è½¬æ¢ä¸ºå°å†™è¿›è¡Œæ¯”è¾ƒï¼‰ã€‚æœ€åŽï¼Œä½¿ç”¨`join()`æ–¹æ³•å°†è¿‡æ»¤åŽçš„å•è¯åˆ—è¡¨é‡æ–°ç»„åˆæˆä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œå¹¶æ‰“å°ç»“æžœã€‚

**æ³¨æ„ï¼š** ä¸Šè¿°ä»£ç å‡ä¸ºç¤ºä¾‹ï¼Œä¸Žæœ¬é¢˜æ— å…³ï¼Œå®žé™…åœ¨å®Œæˆæœ¬é¢˜è¿‡ç¨‹ä¸­ï¼Œéœ€è¦æ ¹æ®å…·ä½“ä»»åŠ¡å’Œæ•°æ®è¿›è¡Œç›¸åº”çš„è°ƒæ•´å’Œä¿®æ”¹ã€‚

## é¢˜ç›®-æƒ…ç»ªåˆ†ç±»

* æœ¬é¢˜æ—¨åœ¨è®­ç»ƒä¸€ä¸ªæ–‡æœ¬äºŒåˆ†ç±»æ¨¡åž‹ï¼Œç”¨ä»¥é¢„æµ‹æ¯ä¸ªå¥å­æ‰€è¡¨è¾¾çš„æƒ…ç»ªæ˜¯æ­£é¢è¿˜æ˜¯è´Ÿé¢
* å¯ä»¥ä½¿ç”¨ MLP æˆ–è€… RNN ç±»æ¨¡åž‹ç­‰ï¼Œä¸å¼ºåˆ¶ä½¿ç”¨ä»»ä½•æ¨¡åž‹æˆ–å®žçŽ°æ–¹æ³•ï¼Œè‡ªç”±å‘æŒ¥ï¼
* å¯ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¡†æž¶ï¼Œæ¡†æž¶ä¸é™ï¼ŒæŽ¨èä½¿ç”¨ tensorflow2 æˆ– pytorch

## ä¸¾ä¸ªæ —å­ðŸŒ°

**ä¸‹é¢æ˜¯ä¸€ä¸ªä½¿ç”¨** **pytorch** **æ¡†æž¶+ LSTM æ¨¡åž‹çš„ä»£ç ç¤ºä¾‹**

### **çŽ¯å¢ƒé…ç½®**

1. å®‰è£… Anaconda,å­¦ä¹ ç®€å•çš„ conda å‘½ä»¤
2. æŽ¨èé€‰æ‹© tensorflow æˆ–è€… pytorch æ¡†æž¶ï¼Œåˆ™åˆ›å»ºç›¸åº”çŽ¯å¢ƒï¼Œå®‰è£… tensorflow/pytorchï¼Œä»¥åŠ GPU é…ç½®ï¼ˆå»ºè®®å®‰è£…ï¼Œéœ€è¦ç”µè„‘æœ‰ç‹¬æ˜¾ï¼‰

### **æ•°æ®å¤„ç†**

- èŽ·å–æ•°æ®é›†ï¼ˆæ‹›æ–°ç¾¤ 683234808ï¼‰æˆ–è€…ä»Ž kaggle ä¸Šä¸‹è½½ [ML2020spring - hw4 | Kaggle](https://www.kaggle.com/competitions/ml2020spring-hw4/overview)
- **åœ¨è¯¥æ•°æ®é›†ä¸­åªä¼šç”¨åˆ° training_label.txtï¼Œå¹¶ä¸”è¯·è‡ªè¡Œåˆ’åˆ†è®­ç»ƒã€éªŒè¯ã€æµ‹è¯•é›†**
- è®­ç»ƒé›†çš„æ¯ä¸€è¡Œä¸ºä¸€ä¸ªæ•°æ®ï¼Œæ¯ä¸ªæ•°æ®åˆ†åˆ«åŒ…å«ä¸€ä¸ªæ–‡æœ¬å’Œä¸€ä¸ª labelï¼ˆ0è¡¨ç¤ºæ¶ˆæžæƒ…ç»ªï¼Œ1è¡¨ç¤ºç§¯æžæƒ…ç»ªï¼‰
- æ‹¿åˆ°æ•°æ®ä¹‹åŽï¼Œéœ€è¦å¯¹æ•°æ®è¿›è¡Œæ–‡æœ¬å¤„ç†ï¼ˆå¯å‚è€ƒä¸Šæ–‡ï¼‰ï¼Œå†å»ºç«‹è¯è¡¨ï¼Œä»¥åŠåš word2idã€label2id çš„æ˜ å°„ç­‰ï¼š

```Python
def create_corpus(texts):
    """
    é€šè¿‡å¯¹è®­ç»ƒæ–‡æœ¬åšæ–‡æœ¬å¤„ç†ï¼ˆå‚è€ƒä¸Šæ–‡ï¼‰å¹¶ç»Ÿè®¡åˆ†è¯ç»“æžœï¼Œå»ºç«‹è¯è¡¨
    å°Tipsï¼šå»ºç«‹è¯è¡¨çš„æ—¶å€™åˆ«å¿˜äº†åŠ ä¸€äº›ç‰¹æ®Štokenï¼Œå¦‚ï¼š
        [PAD]ï¼šç”¨äºŽidåºåˆ—çš„paddingçš„token
        [UNK]ï¼šç”¨äºŽæ˜ å°„ä¸å­˜åœ¨äºŽè¯è¡¨çš„è¯
    è¯·è‡ªè¡Œå®žçŽ°
    """

def preprocess_text(text):
    """
    å¯¹æ•°æ®è¿›è¡Œå¤„ç†ï¼ŒåŒ…æ‹¬æ–‡æœ¬å¤„ç†ï¼ˆå‚è€ƒä¸Šæ–‡ï¼‰ã€åšword2idæ˜ å°„ã€idåºåˆ—çš„paddingç­‰
    paddingï¼š
        ç”±äºŽå°†ä¸åŒçš„æ–‡æœ¬åºåˆ—ç»„æˆä¸€ä¸ªbatchæ—¶ï¼Œéœ€è¦æ¯ä¸ªæ–‡æœ¬çš„é•¿åº¦ç›¸åŒ
        æ‰€ä»¥éœ€è¦å¯¹æŸäº›æ–‡æœ¬è¿›è¡Œæˆªæ–­æˆ–å¡«å……ï¼Œå¡«å……çš„wordå³ä¸º[PAD]
        å¦‚ï¼šå°†I love youå¡«å……è‡³seq_len=5ï¼Œå³ä¸ºI love you [PAD] [PAD]
    è¯·è‡ªè¡Œå®žçŽ°
    """
    
def preprocess_label(label):
    """
    åŒ…æ‹¬label2idçš„æ˜ å°„ç­‰
    è¯·è‡ªè¡Œå®žçŽ°
    """
```

### **Dataset**

- åœ¨pytorchä¸­ï¼Œéœ€è¦å»ºç«‹`Dataset()`å®šä¹‰æˆ‘ä»¬å¤„ç†æ•°æ®çš„æ–¹å¼ï¼Œå…¶ä¸­çš„`__getitem()__`å³å®šä¹‰äº†æ¯å•ä¸ªæ•°æ®çš„å¤„ç†æ–¹æ³•ï¼Œä¹‹åŽåœ¨è®­ç»ƒæ—¶å†ç”¨`DataLoader()`è¿›è¡Œå°è£…ï¼Œå³å¯ç»„æˆä¸€ä¸ª batch çš„æ•°æ®

```Python
class CustomDataset(torch.utils.data.Dataset):
    def __init__(self, texts, labels):
        self.texts = texts
        self.labels = labels

    def __getitem__(self, index):
        text = preprocess_text(self.texts[index])
        label = preprocess_label(self.labels[index])
        
        return text, label

    def __len__(self):
        return len(self.texts)
```

### **æž„é€ æ¨¡åž‹**

- æŽ¥ä¸‹æ¥å°±æ˜¯å¦‚ä½•æ­å»ºæ¨¡åž‹ï¼Œè¿™é‡Œä½¿ç”¨ LSTM åšå®žä¾‹ï¼Œå¤§å®¶ä¹Ÿå¯ä»¥å°½æƒ…å°è¯•å…¶ä»–æ¨¡åž‹
- â—**è¯·ä¸€å®šä¿è¯ä½ ä½¿ç”¨çš„æ¨¡åž‹æ˜¯ä½ ä¼šçš„ï¼Œæ¯”å¦‚å¦‚æžœä½ ç”¨äº† LSTMï¼Œé‚£ä¹ˆæˆ‘ä»¬åˆ™ä¼šåœ¨é¢è¯•æ—¶æŠ½æŸ¥ LSTM ç›¸å…³çš„çŸ¥è¯†**â—
- æ–‡æœ¬ä»»åŠ¡éƒ½ä¼šæ¶‰åŠåˆ° embeddingï¼Œå…¶æœ¬è´¨æ˜¯å­˜å‚¨äº†ä¸€ä¸ªå‘é‡çŸ©é˜µï¼Œè¯¥çŸ©é˜µ`shape=(vocab_size, embedding_dim)`ï¼Œåœ¨å‰å‘ä¼ æ’­æ—¶ï¼Œä¼šå°†æ¯ä¸ªè¯éƒ½æ˜ å°„ä¸ºä¸€ä¸ª embedding_dim ç»´çš„å‘é‡

```Python
class LSTMModel(torch.nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout=0.1):
        super(LSTMModel, self).__init__()
        # embeddingæ˜¯å°†æŸä¸ªè¯å¯¹åº”çš„idæ˜ å°„åˆ°å‘é‡ï¼Œç„¶åŽç”¨è¿™ä¸ªå‘é‡ä½œä¸ºæ¨¡åž‹çš„è¾“å…¥ï¼Œè¯¥å‘é‡çš„ç»´åº¦ä¸ºembedding_dim
        self.embedding = torch.nn.Embedding(vocab_size, embedding_dim)
        # è¿™é‡Œç”¨çš„LSTMæ¨¡åž‹ï¼Œä¹Ÿå¯ä»¥å°è¯•å…¶ä»–æ¨¡åž‹
        self.LSTM = torch.nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)
        self.classifier = torch.nn.Sequential(
            torch.nn.Dropout(dropout),
            torch.nn.Linear(hidden_dim, 2)
        )

    def forward(self, inputs):
        # æœ€å¼€å§‹çš„è¾“å…¥inputs.shape = (batch_size, seq_len)
        inputs = self.embedding(inputs)
        # è¿‡äº†self.embddingä¹‹åŽï¼Œinputs.shape = (batch_size, seq_len, embedding_dim)
        x, _ = self.LSTM(inputs)
        # x.shape = (batch_size, seq_len, hidden_size)
        # å–ç”¨ LSTM æœ€åŽä¸€ä¸ªçš„ hidden state
        x = x[:, -1, :]
        x = self.classifier(x)

        return x
```

### **è®­ç»ƒæ¨¡åž‹**

- é¦–å…ˆå®šä¹‰éœ€è¦çš„ç»„ä»¶ï¼Œè¿™é‡ŒæŸå¤±é‡‡ç”¨äº¤å‰ç†µæŸå¤±ï¼Œä¼˜åŒ–å™¨é‡‡ç”¨ Adam

```Python
model = LSTMModel(
        vocab_size=30000,   # vocab_sizeä¸ºè¯è¡¨å¤§å°
        embedding_dim=300,
        hidden_dim=256,
        num_layers=1,
        dropout=0.1,
).cuda()    # å°†æ¨¡åž‹ç§»å…¥GPUï¼Œåªæœ‰ç”¨GPUæ—¶æ‰éœ€è¦.cuda()
# å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
```

- æŽ¥ä¸‹æ¥å°±æ˜¯å¦‚ä½•è¯»å–æ•°æ®ï¼Œå¹¶ç»„æˆ batch çš„æ•°æ®ï¼Œéœ€è¦ç”¨åˆ°ä¸Šæ–‡çš„`CustomDataset()`

```Python
train_dataset = CustomDataset(texts_train, labels_train)
train_loader = torch.utils.data.DataLoader(dataset=train_dataset,
     batch_size=batch_size, shuffle=True)
```

- ç„¶åŽå°±å¯ä»¥å¼€å§‹è®­ç»ƒäº†

```Python
for epoch in range(num_epochs):
    for inputs, labels in train_loader:
        # å°†tensorç§»å…¥GPU
        inputs = inputs.cuda()
        labels = labels.cuda()
        
        optimizer.zero_grad()   # å°†ä¸Šä¸€æ¬¡è¿­ä»£çš„æ¢¯åº¦æ¸…å‡‰
        outputs = model(inputs)   # å‰å‘ä¼ æ’­
        loss = criterion(outputs, labels)   # è®¡ç®—æŸå¤±
        loss.backward()    # åå‘ä¼ æ’­ï¼ŒèŽ·å¾—æ¢¯åº¦
        optimizer.step()    # é€šè¿‡æ¢¯åº¦ï¼Œè¿›è¡Œå‚æ•°æ›´æ–°
        
# è®­ç»ƒå®Œæ¯•ï¼Œä¿å­˜æ¨¡åž‹
torch.save(model, "./model.pt")
```

### **è¯„ä¼°æ¨¡åž‹**

- è¯„ä¼°æ¨¡åž‹ï¼Œæˆ‘ä»¬è¿˜éœ€è¦åˆ’åˆ†å‡ºä¸€ä¸ªéªŒè¯/æµ‹è¯•é›†ï¼Œå¹¶ä¸”åŒæ ·å»ºç«‹`DataLoader()`ï¼Œå‡è®¾ä¸º`val_loader`

```Python
# åŠ è½½æ¨¡åž‹
model = torch.load("./model.pt")

for inputs, labels in val_loader:
    outputs = model(inputs)
    # çŽ°åœ¨çš„outputsä¸ºä¸€ä¸ªbatchçš„logitsï¼Œå¹¶ä¸”shape=(batch_size, 2)
    # è¯·ä½ é€šè¿‡è¿™ä¸ªoutputsçŸ©é˜µï¼Œç»“åˆä½ æ‰€æŒ‘é€‰çš„æ¨¡åž‹è¯„ä¼°æŒ‡æ ‡ï¼ˆå¦‚å‡†ç¡®çŽ‡ï¼‰ï¼Œå¯¹æ¨¡åž‹è¿›è¡Œè¯„ä¼°
    # è¯·è‡ªè¡Œå®žçŽ°
```

**æ³¨æ„**ï¼Œ**ä¸Šè¿°å‡åªæ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼Œå®žé™…åœ¨å®Œæˆæœ¬é¢˜è¿‡ç¨‹ä¸­ï¼Œéœ€è¦æ ¹æ®å…·ä½“ä»»åŠ¡å’Œæ•°æ®è¿›è¡Œç›¸åº”çš„è°ƒæ•´å’Œä¿®æ”¹ã€‚**

## æ€è€ƒ

1. ä½ ä½¿ç”¨çš„æ˜¯ä»€ä¹ˆæŸå¤±å‡½æ•°ï¼Ÿè¯·ç®€å•ä»‹ç»è¿™ä¸ªæŸå¤±å‡½æ•°ï¼Œå¦‚æžœæœ‰æ¦‚çŽ‡è®ºåŸºç¡€å¹¶äº†è§£æœ€å¤§ä¼¼ç„¶çš„åŒå­¦ï¼Œè¯·å°è¯•æŽ¨å¯¼å‡ºæŸå¤±å‡½æ•°çš„æ•°å­¦å½¢å¼ã€‚
2. åœ¨æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸­å¯èƒ½ä¼šé¢ä¸´è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œå°¤å…¶æ˜¯å½“è®­ç»ƒæ•°æ®è¾ƒå°‘æ—¶ï¼Œå¯ä»¥é‡‡ç”¨å“ªäº›å¸¸è§çš„é˜²æ­¢è¿‡æ‹Ÿåˆçš„æ–¹æ³•ã€‚
3. åå‘ä¼ æ’­æ˜¯ä¸€ç§ç”¨äºŽè®­ç»ƒç¥žç»ç½‘ç»œæ¨¡åž‹çš„ç®—æ³•ï¼Œé€šè¿‡è®¡ç®—æŸå¤±å‡½æ•°å¯¹æ¨¡åž‹å‚æ•°çš„æ¢¯åº¦ï¼Œç„¶åŽåˆ©ç”¨æ¢¯åº¦ä¸‹é™æ³•æ›´æ–°æ¨¡åž‹å‚æ•°ã€‚è¯·æ€è€ƒå¹¶ç®€å•æŽ¨å¯¼ä¸€ä¸‹**ä½ æ‰€ç”¨çš„è¿™ä¸ªæ¨¡åž‹çš„**åå‘ä¼ æ’­å…¬å¼ã€‚
4. åœ¨ LSTM ä¸­ï¼Œè¾“å…¥é—¨ã€é—å¿˜é—¨å’Œè¾“å‡ºé—¨æ˜¯å¦‚ä½•å®žçŽ°å…¶åŠŸèƒ½çš„ã€‚

## å›žç­”è¦æ±‚

1. å¤„ç†æ•°æ®ã€è®­ç»ƒå’Œæµ‹è¯•çš„ä»£ç ï¼Œå¹¶å¤§è‡´è§£é‡Šä½ çš„ä»£ç ï¼›
2. ä»£ç è¿è¡Œç»“æžœæˆªå›¾å’Œè®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒæŸå¤±å’Œ**è¯„ä¼°æŒ‡æ ‡**ï¼ˆ**è¯„ä¼°æŒ‡æ ‡ä¸é™ï¼Œå¦‚å‡†ç¡®çŽ‡**ï¼‰çš„å˜åŒ–å›¾åƒï¼Œå¹¶åœ¨æœ€åŽä½¿ç”¨ä½ æ‰€åˆ’åˆ†çš„æµ‹è¯•é›†å’Œä½ é€‰æ‹©çš„è¯„ä¼°æŒ‡æ ‡è¯„ä¼°æ¨¡åž‹ç»“æžœï¼›
3. å¿…è¦çš„æ³¨é‡Šè¯´æ˜ŽåŠè‰¯å¥½çš„ä»£ç è§„èŒƒï¼›
4. å®žçŽ°æ€è·¯å’Œå­¦åˆ°çš„çŸ¥è¯†ç‚¹ã€‚

## æœ¬é¢˜æäº¤æ–¹å¼

> æ”¶ä»¶é‚®ç®±ï¼šglimmer401@outlook.com  
>
> ä¸»é¢˜æ ¼å¼ï¼šå­¦å·-å§“å-è€ƒæ ¸-æœºå™¨å­¦ä¹ -03
>
> ä¸»é¢˜ç¤ºä¾‹ï¼š2023091202014-å¼ ä¸‰-è€ƒæ ¸-æœºå™¨å­¦ä¹ -03

> å‡ºé¢˜äººQQï¼š674940575
>
> å‡ºé¢˜äººé‚®ç®±ï¼š[674940575@qq.com](mailto:674940575@qq.com)




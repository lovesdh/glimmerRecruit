import{_ as s,o as n,c as a,Q as l}from"./chunks/framework.e90f0c97.js";const u=JSON.parse('{"title":"机器学习-03：文本分类任务","description":"","frontmatter":{},"headers":[],"relativePath":"md/机器学习/机器学习方向-03：文本分类.md","filePath":"md/机器学习/机器学习方向-03：文本分类.md"}'),p={name:"md/机器学习/机器学习方向-03：文本分类.md"},o=l(`<p><img src="https://img1.imgtp.com/2024/09/10/081jz2kT.PNG" alt=""></p><h1 id="机器学习-03-文本分类任务" tabindex="-1"><strong>机器学习</strong>-03：文本分类任务 <a class="header-anchor" href="#机器学习-03-文本分类任务" aria-label="Permalink to &quot;**机器学习**-03：文本分类任务&quot;">​</a></h1><blockquote><p><code>难度系数</code>：中等</p><p>在学习了有关梯度下降的知识之后，可以开始尝试深度学习框架了！深度学习框架是用于构建、训练和部署深度学习模型的软件工具。它们提供了一系列的函数、类和工具，使得开发者可以方便地定义、优化和执行深度学习模型。常见的深度学习框架有tensorflow、pytorch、keras等，选择一种框架进行学习，在学习过程中记录下你的思考与体验。</p></blockquote><h2 id="前置知识" tabindex="-1">前置知识 <a class="header-anchor" href="#前置知识" aria-label="Permalink to &quot;前置知识&quot;">​</a></h2><h3 id="nlp前置知识" tabindex="-1">NLP前置知识 <a class="header-anchor" href="#nlp前置知识" aria-label="Permalink to &quot;NLP前置知识&quot;">​</a></h3><p>在理解和应用 RNN 类模型进行文本分类之前，有几个重要的 NLP 前置知识需要了解：</p><blockquote><p><strong>文本分类：</strong> 文本分类是一种自然语言处理（NLP）任务，旨在将给定的文本分配到预定义的类别或标签中。它是根据文本的内容和语义特征来判断文本所属类别的过程。在文本分类任务中，通常有一个已知的类别集合，每个类别代表一个特定的主题、情感或类别。模型的目标是根据文本的特征和上下文信息，将其正确地分类到相应的类别中。</p><p><strong>文本预处理：</strong> 包括文本清洗、分词、去除停用词等，以便将文本数据转化为可供模型处理的形式。</p><p><strong>词嵌入</strong>（Word Embeddings）：将文本中的单词映射为低维的实数向量表示，以捕捉单词之间的语义关系。</p><p><strong>循环神经网络</strong>（Recurrent Neural Networks，RNN）：一种能够处理序列数据的神经网络模型，通过在每个时间步骤上传递隐藏状态，可以捕捉到序列中的上下文信息。</p><p><strong>长短时记忆网络</strong>（Long Short-Term Memory，LSTM）：一种RNN的变体，通过引入门控机制，可以更好地处理长期依赖关系。</p><p><strong>注意⭐️：在本次任务中，我们并不限定任何模型和实现方式。RNN和LSTM不是必须的，推荐使用，还未接触RNN的初学者同学也可以使用MLP模型（是的MLP也是可以用的，只是效果差点，但是我们强烈推荐初学者同学用MLP来一次实践），高阶的同学甚至可以直接上BERT模型</strong></p></blockquote><h3 id="文本处理前置知识" tabindex="-1">文本处理前置知识 <a class="header-anchor" href="#文本处理前置知识" aria-label="Permalink to &quot;文本处理前置知识&quot;">​</a></h3><p>文本处理是指对文本数据进行<strong>预处理和转换</strong>的过程，以便于后续的文本分析、挖掘和建模。文本处理的目标是清洗、规范和转换原始文本数据，以提取有用的信息和特征，从而支持各种文本相关的任务，如文本分类、情感分析、机器翻译等。</p><p>下面是一些常见的文本处理步骤：</p><ol><li>文本清洗：去除文本中的特殊字符、标点符号和HTML标签等无关信息。处理大小写，可以将文本转换为小写形式，以避免同一个单词因大小写不同而被视为不同的词汇。</li></ol><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> re</span></span>
<span class="line"><span style="color:#F97583;">def</span><span style="color:#E1E4E8;"> </span><span style="color:#B392F0;">clean_text</span><span style="color:#E1E4E8;">(text):</span></span>
<span class="line"><span style="color:#E1E4E8;">    </span><span style="color:#6A737D;"># 去除特殊字符和标点符号</span></span>
<span class="line"><span style="color:#E1E4E8;">    text </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> re.sub(</span><span style="color:#F97583;">r</span><span style="color:#9ECBFF;">&quot;</span><span style="color:#79B8FF;">[</span><span style="color:#F97583;">^</span><span style="color:#79B8FF;">a-zA-Z0-9]</span><span style="color:#9ECBFF;">&quot;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&quot; &quot;</span><span style="color:#E1E4E8;">, text)</span></span>
<span class="line"><span style="color:#E1E4E8;">    </span><span style="color:#6A737D;"># 将文本转换为小写</span></span>
<span class="line"><span style="color:#E1E4E8;">    text </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> text.lower()</span></span>
<span class="line"><span style="color:#E1E4E8;">    </span><span style="color:#6A737D;"># 去除多余的空格</span></span>
<span class="line"><span style="color:#E1E4E8;">    text </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> re.sub(</span><span style="color:#F97583;">r</span><span style="color:#9ECBFF;">&quot;</span><span style="color:#79B8FF;">\\s</span><span style="color:#F97583;">+</span><span style="color:#9ECBFF;">&quot;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&quot; &quot;</span><span style="color:#E1E4E8;">, text)</span></span>
<span class="line"><span style="color:#E1E4E8;">    </span><span style="color:#F97583;">return</span><span style="color:#E1E4E8;"> text</span></span>
<span class="line"><span style="color:#6A737D;"># 示例文本</span></span>
<span class="line"><span style="color:#E1E4E8;">text </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> </span><span style="color:#9ECBFF;">&quot;Hello, this is an example text! It contains special characters and punctuation.&quot;</span></span>
<span class="line"><span style="color:#6A737D;"># 清洗文本</span></span>
<span class="line"><span style="color:#E1E4E8;">cleaned_text </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> clean_text(text)</span></span>
<span class="line"><span style="color:#79B8FF;">print</span><span style="color:#E1E4E8;">(cleaned_text)</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#D73A49;">import</span><span style="color:#24292E;"> re</span></span>
<span class="line"><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">clean_text</span><span style="color:#24292E;">(text):</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#6A737D;"># 去除特殊字符和标点符号</span></span>
<span class="line"><span style="color:#24292E;">    text </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> re.sub(</span><span style="color:#D73A49;">r</span><span style="color:#032F62;">&quot;</span><span style="color:#005CC5;">[</span><span style="color:#D73A49;">^</span><span style="color:#005CC5;">a-zA-Z0-9]</span><span style="color:#032F62;">&quot;</span><span style="color:#24292E;">, </span><span style="color:#032F62;">&quot; &quot;</span><span style="color:#24292E;">, text)</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#6A737D;"># 将文本转换为小写</span></span>
<span class="line"><span style="color:#24292E;">    text </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> text.lower()</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#6A737D;"># 去除多余的空格</span></span>
<span class="line"><span style="color:#24292E;">    text </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> re.sub(</span><span style="color:#D73A49;">r</span><span style="color:#032F62;">&quot;</span><span style="color:#005CC5;">\\s</span><span style="color:#D73A49;">+</span><span style="color:#032F62;">&quot;</span><span style="color:#24292E;">, </span><span style="color:#032F62;">&quot; &quot;</span><span style="color:#24292E;">, text)</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">return</span><span style="color:#24292E;"> text</span></span>
<span class="line"><span style="color:#6A737D;"># 示例文本</span></span>
<span class="line"><span style="color:#24292E;">text </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#032F62;">&quot;Hello, this is an example text! It contains special characters and punctuation.&quot;</span></span>
<span class="line"><span style="color:#6A737D;"># 清洗文本</span></span>
<span class="line"><span style="color:#24292E;">cleaned_text </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> clean_text(text)</span></span>
<span class="line"><span style="color:#005CC5;">print</span><span style="color:#24292E;">(cleaned_text)</span></span></code></pre></div><p>在以上代码中，<code>clean_text</code>函数使用正则表达式去除了特殊字符和标点符号，将文本转换为小写，并去除了多余的空格。你可以根据需要进行修改和扩展，例如添加停用词移除、词干提取等其他文本清洗步骤。</p><ol start="2"><li>分词（Tokenization）：将文本分割成单词或子词的序列。</li></ol><div class="language-Python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">Python</span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#F97583;">import</span><span style="color:#E1E4E8;"> re</span></span>
<span class="line"><span style="color:#F97583;">def</span><span style="color:#E1E4E8;"> </span><span style="color:#B392F0;">tokenize</span><span style="color:#E1E4E8;">(text):</span></span>
<span class="line"><span style="color:#E1E4E8;">    </span><span style="color:#6A737D;"># 将文本中的标点符号替换为空格</span></span>
<span class="line"><span style="color:#E1E4E8;">    text </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> re.sub(</span><span style="color:#F97583;">r</span><span style="color:#9ECBFF;">&#39;</span><span style="color:#79B8FF;">[</span><span style="color:#F97583;">^</span><span style="color:#79B8FF;">\\w\\s]</span><span style="color:#9ECBFF;">&#39;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&#39; &#39;</span><span style="color:#E1E4E8;">, text)</span></span>
<span class="line"><span style="color:#E1E4E8;">    </span><span style="color:#6A737D;"># 将文本按空格分割成单词列表</span></span>
<span class="line"><span style="color:#E1E4E8;">    tokens </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> text.split()</span></span>
<span class="line"><span style="color:#E1E4E8;">    </span><span style="color:#F97583;">return</span><span style="color:#E1E4E8;"> tokens</span></span>
<span class="line"><span style="color:#6A737D;"># 示例文本</span></span>
<span class="line"><span style="color:#E1E4E8;">text </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> </span><span style="color:#9ECBFF;">&quot;This is an example sentence for tokenization.&quot;</span></span>
<span class="line"><span style="color:#6A737D;"># 分词</span></span>
<span class="line"><span style="color:#E1E4E8;">tokens </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> tokenize(text)</span></span>
<span class="line"><span style="color:#79B8FF;">print</span><span style="color:#E1E4E8;">(tokens)</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#D73A49;">import</span><span style="color:#24292E;"> re</span></span>
<span class="line"><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">tokenize</span><span style="color:#24292E;">(text):</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#6A737D;"># 将文本中的标点符号替换为空格</span></span>
<span class="line"><span style="color:#24292E;">    text </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> re.sub(</span><span style="color:#D73A49;">r</span><span style="color:#032F62;">&#39;</span><span style="color:#005CC5;">[</span><span style="color:#D73A49;">^</span><span style="color:#005CC5;">\\w\\s]</span><span style="color:#032F62;">&#39;</span><span style="color:#24292E;">, </span><span style="color:#032F62;">&#39; &#39;</span><span style="color:#24292E;">, text)</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#6A737D;"># 将文本按空格分割成单词列表</span></span>
<span class="line"><span style="color:#24292E;">    tokens </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> text.split()</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">return</span><span style="color:#24292E;"> tokens</span></span>
<span class="line"><span style="color:#6A737D;"># 示例文本</span></span>
<span class="line"><span style="color:#24292E;">text </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#032F62;">&quot;This is an example sentence for tokenization.&quot;</span></span>
<span class="line"><span style="color:#6A737D;"># 分词</span></span>
<span class="line"><span style="color:#24292E;">tokens </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> tokenize(text)</span></span>
<span class="line"><span style="color:#005CC5;">print</span><span style="color:#24292E;">(tokens)</span></span></code></pre></div><ol start="3"><li>去除停用词（Stop Words）：停用词是在文本中频繁出现但通常不携带重要信息的常见词汇，如介词、连词和冠词等。去除停用词可以减少文本的维度，并提高模型的效果。</li></ol><div class="language-Python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">Python</span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#6A737D;"># 停用词列表</span></span>
<span class="line"><span style="color:#E1E4E8;">stop_words </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> [</span><span style="color:#9ECBFF;">&quot;the&quot;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&quot;is&quot;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&quot;an&quot;</span><span style="color:#E1E4E8;">, </span><span style="color:#9ECBFF;">&quot;of&quot;</span><span style="color:#E1E4E8;">]</span></span>
<span class="line"><span style="color:#6A737D;"># 示例文本</span></span>
<span class="line"><span style="color:#E1E4E8;">text </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> </span><span style="color:#9ECBFF;">&quot;This is an example sentence demonstrating the removal of stopwords.&quot;</span></span>
<span class="line"><span style="color:#6A737D;"># 分词</span></span>
<span class="line"><span style="color:#E1E4E8;">tokens </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> text.split()</span></span>
<span class="line"><span style="color:#6A737D;"># 去除停用词</span></span>
<span class="line"><span style="color:#E1E4E8;">filtered_tokens </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> [word </span><span style="color:#F97583;">for</span><span style="color:#E1E4E8;"> word </span><span style="color:#F97583;">in</span><span style="color:#E1E4E8;"> tokens </span><span style="color:#F97583;">if</span><span style="color:#E1E4E8;"> word.lower() </span><span style="color:#F97583;">not</span><span style="color:#E1E4E8;"> </span><span style="color:#F97583;">in</span><span style="color:#E1E4E8;"> stop_words]</span></span>
<span class="line"><span style="color:#6A737D;"># 打印结果</span></span>
<span class="line"><span style="color:#79B8FF;">print</span><span style="color:#E1E4E8;">(</span><span style="color:#9ECBFF;">&quot;Original Text:&quot;</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#79B8FF;">print</span><span style="color:#E1E4E8;">(text)</span></span>
<span class="line"><span style="color:#79B8FF;">print</span><span style="color:#E1E4E8;">()</span></span>
<span class="line"><span style="color:#79B8FF;">print</span><span style="color:#E1E4E8;">(</span><span style="color:#9ECBFF;">&quot;Filtered Text:&quot;</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#79B8FF;">print</span><span style="color:#E1E4E8;">(</span><span style="color:#9ECBFF;">&#39; &#39;</span><span style="color:#E1E4E8;">.join(filtered_tokens))</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#6A737D;"># 停用词列表</span></span>
<span class="line"><span style="color:#24292E;">stop_words </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> [</span><span style="color:#032F62;">&quot;the&quot;</span><span style="color:#24292E;">, </span><span style="color:#032F62;">&quot;is&quot;</span><span style="color:#24292E;">, </span><span style="color:#032F62;">&quot;an&quot;</span><span style="color:#24292E;">, </span><span style="color:#032F62;">&quot;of&quot;</span><span style="color:#24292E;">]</span></span>
<span class="line"><span style="color:#6A737D;"># 示例文本</span></span>
<span class="line"><span style="color:#24292E;">text </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#032F62;">&quot;This is an example sentence demonstrating the removal of stopwords.&quot;</span></span>
<span class="line"><span style="color:#6A737D;"># 分词</span></span>
<span class="line"><span style="color:#24292E;">tokens </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> text.split()</span></span>
<span class="line"><span style="color:#6A737D;"># 去除停用词</span></span>
<span class="line"><span style="color:#24292E;">filtered_tokens </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> [word </span><span style="color:#D73A49;">for</span><span style="color:#24292E;"> word </span><span style="color:#D73A49;">in</span><span style="color:#24292E;"> tokens </span><span style="color:#D73A49;">if</span><span style="color:#24292E;"> word.lower() </span><span style="color:#D73A49;">not</span><span style="color:#24292E;"> </span><span style="color:#D73A49;">in</span><span style="color:#24292E;"> stop_words]</span></span>
<span class="line"><span style="color:#6A737D;"># 打印结果</span></span>
<span class="line"><span style="color:#005CC5;">print</span><span style="color:#24292E;">(</span><span style="color:#032F62;">&quot;Original Text:&quot;</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#005CC5;">print</span><span style="color:#24292E;">(text)</span></span>
<span class="line"><span style="color:#005CC5;">print</span><span style="color:#24292E;">()</span></span>
<span class="line"><span style="color:#005CC5;">print</span><span style="color:#24292E;">(</span><span style="color:#032F62;">&quot;Filtered Text:&quot;</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#005CC5;">print</span><span style="color:#24292E;">(</span><span style="color:#032F62;">&#39; &#39;</span><span style="color:#24292E;">.join(filtered_tokens))</span></span></code></pre></div><p>在上述代码中，首先定义了一个停用词列（<code>stop_words</code>），其中包含要去除的常见停用词。定义了一个示例文本（<code>text</code>）。接下来，使用<code>split()</code>方法将文本分割成单词，得到一个单词列表（<code>tokens</code>）。然后，使用列表推导式，遍历分词后的单词列表，并将不在停用词列表中的单词保留下来（使用<code>lower()</code>方法将单词转换为小写进行比较）。最后，使用<code>join()</code>方法将过滤后的单词列表重新组合成一个字符串，并打印结果。</p><p><strong>注意：</strong> 上述代码均为示例，与本题无关，实际在完成本题过程中，需要根据具体任务和数据进行相应的调整和修改。</p><h2 id="题目-情绪分类" tabindex="-1">题目-情绪分类 <a class="header-anchor" href="#题目-情绪分类" aria-label="Permalink to &quot;题目-情绪分类&quot;">​</a></h2><ul><li>本题旨在训练一个文本二分类模型，用以预测每个句子所表达的情绪是正面还是负面</li><li>可以使用 MLP 或者 RNN 类模型等，不强制使用任何模型或实现方法，自由发挥！</li><li>可使用深度学习框架，框架不限，推荐使用 tensorflow2 或 pytorch</li></ul><h2 id="举个栗子🌰" tabindex="-1">举个栗子🌰 <a class="header-anchor" href="#举个栗子🌰" aria-label="Permalink to &quot;举个栗子🌰&quot;">​</a></h2><p><strong>下面是一个使用</strong> <strong>pytorch</strong> <strong>框架+ LSTM 模型的代码示例</strong></p><h3 id="环境配置" tabindex="-1"><strong>环境配置</strong> <a class="header-anchor" href="#环境配置" aria-label="Permalink to &quot;**环境配置**&quot;">​</a></h3><ol><li>安装 Anaconda,学习简单的 conda 命令</li><li>推荐选择 tensorflow 或者 pytorch 框架，则创建相应环境，安装 tensorflow/pytorch，以及 GPU 配置（建议安装，需要电脑有独显）</li></ol><h3 id="数据处理" tabindex="-1"><strong>数据处理</strong> <a class="header-anchor" href="#数据处理" aria-label="Permalink to &quot;**数据处理**&quot;">​</a></h3><ul><li>获取数据集（招新群 683234808）或者从 kaggle 上下载 <a href="https://www.kaggle.com/competitions/ml2020spring-hw4/overview" target="_blank" rel="noreferrer">ML2020spring - hw4 | Kaggle</a></li><li><strong>在该数据集中只会用到 training_label.txt，并且请自行划分训练、验证、测试集</strong></li><li>训练集的每一行为一个数据，每个数据分别包含一个文本和一个 label（0表示消极情绪，1表示积极情绪）</li><li>拿到数据之后，需要对数据进行文本处理（可参考上文），再建立词表，以及做 word2id、label2id 的映射等：</li></ul><div class="language-Python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">Python</span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#F97583;">def</span><span style="color:#E1E4E8;"> </span><span style="color:#B392F0;">create_corpus</span><span style="color:#E1E4E8;">(texts):</span></span>
<span class="line"><span style="color:#E1E4E8;">    </span><span style="color:#9ECBFF;">&quot;&quot;&quot;</span></span>
<span class="line"><span style="color:#9ECBFF;">    通过对训练文本做文本处理（参考上文）并统计分词结果，建立词表</span></span>
<span class="line"><span style="color:#9ECBFF;">    小Tips：建立词表的时候别忘了加一些特殊token，如：</span></span>
<span class="line"><span style="color:#9ECBFF;">        [PAD]：用于id序列的padding的token</span></span>
<span class="line"><span style="color:#9ECBFF;">        [UNK]：用于映射不存在于词表的词</span></span>
<span class="line"><span style="color:#9ECBFF;">    请自行实现</span></span>
<span class="line"><span style="color:#9ECBFF;">    &quot;&quot;&quot;</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583;">def</span><span style="color:#E1E4E8;"> </span><span style="color:#B392F0;">preprocess_text</span><span style="color:#E1E4E8;">(text):</span></span>
<span class="line"><span style="color:#E1E4E8;">    </span><span style="color:#9ECBFF;">&quot;&quot;&quot;</span></span>
<span class="line"><span style="color:#9ECBFF;">    对数据进行处理，包括文本处理（参考上文）、做word2id映射、id序列的padding等</span></span>
<span class="line"><span style="color:#9ECBFF;">    padding：</span></span>
<span class="line"><span style="color:#9ECBFF;">        由于将不同的文本序列组成一个batch时，需要每个文本的长度相同</span></span>
<span class="line"><span style="color:#9ECBFF;">        所以需要对某些文本进行截断或填充，填充的word即为[PAD]</span></span>
<span class="line"><span style="color:#9ECBFF;">        如：将I love you填充至seq_len=5，即为I love you [PAD] [PAD]</span></span>
<span class="line"><span style="color:#9ECBFF;">    请自行实现</span></span>
<span class="line"><span style="color:#9ECBFF;">    &quot;&quot;&quot;</span></span>
<span class="line"><span style="color:#E1E4E8;">    </span></span>
<span class="line"><span style="color:#F97583;">def</span><span style="color:#E1E4E8;"> </span><span style="color:#B392F0;">preprocess_label</span><span style="color:#E1E4E8;">(label):</span></span>
<span class="line"><span style="color:#E1E4E8;">    </span><span style="color:#9ECBFF;">&quot;&quot;&quot;</span></span>
<span class="line"><span style="color:#9ECBFF;">    包括label2id的映射等</span></span>
<span class="line"><span style="color:#9ECBFF;">    请自行实现</span></span>
<span class="line"><span style="color:#9ECBFF;">    &quot;&quot;&quot;</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">create_corpus</span><span style="color:#24292E;">(texts):</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#032F62;">&quot;&quot;&quot;</span></span>
<span class="line"><span style="color:#032F62;">    通过对训练文本做文本处理（参考上文）并统计分词结果，建立词表</span></span>
<span class="line"><span style="color:#032F62;">    小Tips：建立词表的时候别忘了加一些特殊token，如：</span></span>
<span class="line"><span style="color:#032F62;">        [PAD]：用于id序列的padding的token</span></span>
<span class="line"><span style="color:#032F62;">        [UNK]：用于映射不存在于词表的词</span></span>
<span class="line"><span style="color:#032F62;">    请自行实现</span></span>
<span class="line"><span style="color:#032F62;">    &quot;&quot;&quot;</span></span>
<span class="line"></span>
<span class="line"><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">preprocess_text</span><span style="color:#24292E;">(text):</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#032F62;">&quot;&quot;&quot;</span></span>
<span class="line"><span style="color:#032F62;">    对数据进行处理，包括文本处理（参考上文）、做word2id映射、id序列的padding等</span></span>
<span class="line"><span style="color:#032F62;">    padding：</span></span>
<span class="line"><span style="color:#032F62;">        由于将不同的文本序列组成一个batch时，需要每个文本的长度相同</span></span>
<span class="line"><span style="color:#032F62;">        所以需要对某些文本进行截断或填充，填充的word即为[PAD]</span></span>
<span class="line"><span style="color:#032F62;">        如：将I love you填充至seq_len=5，即为I love you [PAD] [PAD]</span></span>
<span class="line"><span style="color:#032F62;">    请自行实现</span></span>
<span class="line"><span style="color:#032F62;">    &quot;&quot;&quot;</span></span>
<span class="line"><span style="color:#24292E;">    </span></span>
<span class="line"><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">preprocess_label</span><span style="color:#24292E;">(label):</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#032F62;">&quot;&quot;&quot;</span></span>
<span class="line"><span style="color:#032F62;">    包括label2id的映射等</span></span>
<span class="line"><span style="color:#032F62;">    请自行实现</span></span>
<span class="line"><span style="color:#032F62;">    &quot;&quot;&quot;</span></span></code></pre></div><h3 id="dataset" tabindex="-1"><strong>Dataset</strong> <a class="header-anchor" href="#dataset" aria-label="Permalink to &quot;**Dataset**&quot;">​</a></h3><ul><li>在pytorch中，需要建立<code>Dataset()</code>定义我们处理数据的方式，其中的<code>__getitem()__</code>即定义了每单个数据的处理方法，之后在训练时再用<code>DataLoader()</code>进行封装，即可组成一个 batch 的数据</li></ul><div class="language-Python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">Python</span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#F97583;">class</span><span style="color:#E1E4E8;"> </span><span style="color:#B392F0;">CustomDataset</span><span style="color:#E1E4E8;">(</span><span style="color:#B392F0;">torch</span><span style="color:#E1E4E8;">.</span><span style="color:#B392F0;">utils</span><span style="color:#E1E4E8;">.</span><span style="color:#B392F0;">data</span><span style="color:#E1E4E8;">.</span><span style="color:#B392F0;">Dataset</span><span style="color:#E1E4E8;">):</span></span>
<span class="line"><span style="color:#E1E4E8;">    </span><span style="color:#F97583;">def</span><span style="color:#E1E4E8;"> </span><span style="color:#79B8FF;">__init__</span><span style="color:#E1E4E8;">(self, texts, labels):</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.texts </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> texts</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.labels </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> labels</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">    </span><span style="color:#F97583;">def</span><span style="color:#E1E4E8;"> </span><span style="color:#79B8FF;">__getitem__</span><span style="color:#E1E4E8;">(self, index):</span></span>
<span class="line"><span style="color:#E1E4E8;">        text </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> preprocess_text(</span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.texts[index])</span></span>
<span class="line"><span style="color:#E1E4E8;">        label </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> preprocess_label(</span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.labels[index])</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#F97583;">return</span><span style="color:#E1E4E8;"> text, label</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">    </span><span style="color:#F97583;">def</span><span style="color:#E1E4E8;"> </span><span style="color:#79B8FF;">__len__</span><span style="color:#E1E4E8;">(self):</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#F97583;">return</span><span style="color:#E1E4E8;"> </span><span style="color:#79B8FF;">len</span><span style="color:#E1E4E8;">(</span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.texts)</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#D73A49;">class</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">CustomDataset</span><span style="color:#24292E;">(</span><span style="color:#6F42C1;">torch</span><span style="color:#24292E;">.</span><span style="color:#6F42C1;">utils</span><span style="color:#24292E;">.</span><span style="color:#6F42C1;">data</span><span style="color:#24292E;">.</span><span style="color:#6F42C1;">Dataset</span><span style="color:#24292E;">):</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">__init__</span><span style="color:#24292E;">(self, texts, labels):</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.texts </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> texts</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.labels </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> labels</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">__getitem__</span><span style="color:#24292E;">(self, index):</span></span>
<span class="line"><span style="color:#24292E;">        text </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> preprocess_text(</span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.texts[index])</span></span>
<span class="line"><span style="color:#24292E;">        label </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> preprocess_label(</span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.labels[index])</span></span>
<span class="line"><span style="color:#24292E;">        </span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#D73A49;">return</span><span style="color:#24292E;"> text, label</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">__len__</span><span style="color:#24292E;">(self):</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#D73A49;">return</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">len</span><span style="color:#24292E;">(</span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.texts)</span></span></code></pre></div><h3 id="构造模型" tabindex="-1"><strong>构造模型</strong> <a class="header-anchor" href="#构造模型" aria-label="Permalink to &quot;**构造模型**&quot;">​</a></h3><ul><li>接下来就是如何搭建模型，这里使用 LSTM 做实例，大家也可以尽情尝试其他模型</li><li>❗<strong>请一定保证你使用的模型是你会的，比如如果你用了 LSTM，那么我们则会在面试时抽查 LSTM 相关的知识</strong>❗</li><li>文本任务都会涉及到 embedding，其本质是存储了一个向量矩阵，该矩阵<code>shape=(vocab_size, embedding_dim)</code>，在前向传播时，会将每个词都映射为一个 embedding_dim 维的向量</li></ul><div class="language-Python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">Python</span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#F97583;">class</span><span style="color:#E1E4E8;"> </span><span style="color:#B392F0;">LSTMModel</span><span style="color:#E1E4E8;">(</span><span style="color:#B392F0;">torch</span><span style="color:#E1E4E8;">.</span><span style="color:#B392F0;">nn</span><span style="color:#E1E4E8;">.</span><span style="color:#B392F0;">Module</span><span style="color:#E1E4E8;">):</span></span>
<span class="line"><span style="color:#E1E4E8;">    </span><span style="color:#F97583;">def</span><span style="color:#E1E4E8;"> </span><span style="color:#79B8FF;">__init__</span><span style="color:#E1E4E8;">(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">0.1</span><span style="color:#E1E4E8;">):</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#79B8FF;">super</span><span style="color:#E1E4E8;">(LSTMModel, </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">).</span><span style="color:#79B8FF;">__init__</span><span style="color:#E1E4E8;">()</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#6A737D;"># embedding是将某个词对应的id映射到向量，然后用这个向量作为模型的输入，该向量的维度为embedding_dim</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.embedding </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torch.nn.Embedding(vocab_size, embedding_dim)</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#6A737D;"># 这里用的LSTM模型，也可以尝试其他模型</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.</span><span style="color:#79B8FF;">LSTM</span><span style="color:#E1E4E8;"> </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torch.nn.LSTM(embedding_dim, hidden_dim, </span><span style="color:#FFAB70;">num_layers</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">num_layers, </span><span style="color:#FFAB70;">batch_first</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">True</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.classifier </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torch.nn.Sequential(</span></span>
<span class="line"><span style="color:#E1E4E8;">            torch.nn.Dropout(dropout),</span></span>
<span class="line"><span style="color:#E1E4E8;">            torch.nn.Linear(hidden_dim, </span><span style="color:#79B8FF;">2</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"><span style="color:#E1E4E8;">        )</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">    </span><span style="color:#F97583;">def</span><span style="color:#E1E4E8;"> </span><span style="color:#B392F0;">forward</span><span style="color:#E1E4E8;">(self, inputs):</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#6A737D;"># 最开始的输入inputs.shape = (batch_size, seq_len)</span></span>
<span class="line"><span style="color:#E1E4E8;">        inputs </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.embedding(inputs)</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#6A737D;"># 过了self.embdding之后，inputs.shape = (batch_size, seq_len, embedding_dim)</span></span>
<span class="line"><span style="color:#E1E4E8;">        x, _ </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.LSTM(inputs)</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#6A737D;"># x.shape = (batch_size, seq_len, hidden_size)</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#6A737D;"># 取用 LSTM 最后一个的 hidden state</span></span>
<span class="line"><span style="color:#E1E4E8;">        x </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> x[:, </span><span style="color:#F97583;">-</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">, :]</span></span>
<span class="line"><span style="color:#E1E4E8;">        x </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> </span><span style="color:#79B8FF;">self</span><span style="color:#E1E4E8;">.classifier(x)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#F97583;">return</span><span style="color:#E1E4E8;"> x</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#D73A49;">class</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">LSTMModel</span><span style="color:#24292E;">(</span><span style="color:#6F42C1;">torch</span><span style="color:#24292E;">.</span><span style="color:#6F42C1;">nn</span><span style="color:#24292E;">.</span><span style="color:#6F42C1;">Module</span><span style="color:#24292E;">):</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">__init__</span><span style="color:#24292E;">(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">0.1</span><span style="color:#24292E;">):</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">super</span><span style="color:#24292E;">(LSTMModel, </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">).</span><span style="color:#005CC5;">__init__</span><span style="color:#24292E;">()</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#6A737D;"># embedding是将某个词对应的id映射到向量，然后用这个向量作为模型的输入，该向量的维度为embedding_dim</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.embedding </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> torch.nn.Embedding(vocab_size, embedding_dim)</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#6A737D;"># 这里用的LSTM模型，也可以尝试其他模型</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.</span><span style="color:#005CC5;">LSTM</span><span style="color:#24292E;"> </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> torch.nn.LSTM(embedding_dim, hidden_dim, </span><span style="color:#E36209;">num_layers</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">num_layers, </span><span style="color:#E36209;">batch_first</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">True</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.classifier </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> torch.nn.Sequential(</span></span>
<span class="line"><span style="color:#24292E;">            torch.nn.Dropout(dropout),</span></span>
<span class="line"><span style="color:#24292E;">            torch.nn.Linear(hidden_dim, </span><span style="color:#005CC5;">2</span><span style="color:#24292E;">)</span></span>
<span class="line"><span style="color:#24292E;">        )</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">def</span><span style="color:#24292E;"> </span><span style="color:#6F42C1;">forward</span><span style="color:#24292E;">(self, inputs):</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#6A737D;"># 最开始的输入inputs.shape = (batch_size, seq_len)</span></span>
<span class="line"><span style="color:#24292E;">        inputs </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.embedding(inputs)</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#6A737D;"># 过了self.embdding之后，inputs.shape = (batch_size, seq_len, embedding_dim)</span></span>
<span class="line"><span style="color:#24292E;">        x, _ </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.LSTM(inputs)</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#6A737D;"># x.shape = (batch_size, seq_len, hidden_size)</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#6A737D;"># 取用 LSTM 最后一个的 hidden state</span></span>
<span class="line"><span style="color:#24292E;">        x </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> x[:, </span><span style="color:#D73A49;">-</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">, :]</span></span>
<span class="line"><span style="color:#24292E;">        x </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">self</span><span style="color:#24292E;">.classifier(x)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#D73A49;">return</span><span style="color:#24292E;"> x</span></span></code></pre></div><h3 id="训练模型" tabindex="-1"><strong>训练模型</strong> <a class="header-anchor" href="#训练模型" aria-label="Permalink to &quot;**训练模型**&quot;">​</a></h3><ul><li>首先定义需要的组件，这里损失采用交叉熵损失，优化器采用 Adam</li></ul><div class="language-Python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">Python</span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#E1E4E8;">model </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> LSTMModel(</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#FFAB70;">vocab_size</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">30000</span><span style="color:#E1E4E8;">,   </span><span style="color:#6A737D;"># vocab_size为词表大小</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#FFAB70;">embedding_dim</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">300</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#FFAB70;">hidden_dim</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">256</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#FFAB70;">num_layers</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">1</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#FFAB70;">dropout</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">0.1</span><span style="color:#E1E4E8;">,</span></span>
<span class="line"><span style="color:#E1E4E8;">).cuda()    </span><span style="color:#6A737D;"># 将模型移入GPU，只有用GPU时才需要.cuda()</span></span>
<span class="line"><span style="color:#6A737D;"># 定义损失函数和优化器</span></span>
<span class="line"><span style="color:#E1E4E8;">criterion </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torch.nn.CrossEntropyLoss()</span></span>
<span class="line"><span style="color:#E1E4E8;">optimizer </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torch.optim.Adam(model.parameters(), </span><span style="color:#FFAB70;">lr</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">0.001</span><span style="color:#E1E4E8;">)</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#24292E;">model </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> LSTMModel(</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#E36209;">vocab_size</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">30000</span><span style="color:#24292E;">,   </span><span style="color:#6A737D;"># vocab_size为词表大小</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#E36209;">embedding_dim</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">300</span><span style="color:#24292E;">,</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#E36209;">hidden_dim</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">256</span><span style="color:#24292E;">,</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#E36209;">num_layers</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">1</span><span style="color:#24292E;">,</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#E36209;">dropout</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">0.1</span><span style="color:#24292E;">,</span></span>
<span class="line"><span style="color:#24292E;">).cuda()    </span><span style="color:#6A737D;"># 将模型移入GPU，只有用GPU时才需要.cuda()</span></span>
<span class="line"><span style="color:#6A737D;"># 定义损失函数和优化器</span></span>
<span class="line"><span style="color:#24292E;">criterion </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> torch.nn.CrossEntropyLoss()</span></span>
<span class="line"><span style="color:#24292E;">optimizer </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> torch.optim.Adam(model.parameters(), </span><span style="color:#E36209;">lr</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">0.001</span><span style="color:#24292E;">)</span></span></code></pre></div><ul><li>接下来就是如何读取数据，并组成 batch 的数据，需要用到上文的<code>CustomDataset()</code></li></ul><div class="language-Python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">Python</span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#E1E4E8;">train_dataset </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> CustomDataset(texts_train, labels_train)</span></span>
<span class="line"><span style="color:#E1E4E8;">train_loader </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torch.utils.data.DataLoader(</span><span style="color:#FFAB70;">dataset</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">train_dataset,</span></span>
<span class="line"><span style="color:#E1E4E8;">     </span><span style="color:#FFAB70;">batch_size</span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;">batch_size, </span><span style="color:#FFAB70;">shuffle</span><span style="color:#F97583;">=</span><span style="color:#79B8FF;">True</span><span style="color:#E1E4E8;">)</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#24292E;">train_dataset </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> CustomDataset(texts_train, labels_train)</span></span>
<span class="line"><span style="color:#24292E;">train_loader </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> torch.utils.data.DataLoader(</span><span style="color:#E36209;">dataset</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">train_dataset,</span></span>
<span class="line"><span style="color:#24292E;">     </span><span style="color:#E36209;">batch_size</span><span style="color:#D73A49;">=</span><span style="color:#24292E;">batch_size, </span><span style="color:#E36209;">shuffle</span><span style="color:#D73A49;">=</span><span style="color:#005CC5;">True</span><span style="color:#24292E;">)</span></span></code></pre></div><ul><li>然后就可以开始训练了</li></ul><div class="language-Python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">Python</span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#F97583;">for</span><span style="color:#E1E4E8;"> epoch </span><span style="color:#F97583;">in</span><span style="color:#E1E4E8;"> </span><span style="color:#79B8FF;">range</span><span style="color:#E1E4E8;">(num_epochs):</span></span>
<span class="line"><span style="color:#E1E4E8;">    </span><span style="color:#F97583;">for</span><span style="color:#E1E4E8;"> inputs, labels </span><span style="color:#F97583;">in</span><span style="color:#E1E4E8;"> train_loader:</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span><span style="color:#6A737D;"># 将tensor移入GPU</span></span>
<span class="line"><span style="color:#E1E4E8;">        inputs </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> inputs.cuda()</span></span>
<span class="line"><span style="color:#E1E4E8;">        labels </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> labels.cuda()</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span></span>
<span class="line"><span style="color:#E1E4E8;">        optimizer.zero_grad()   </span><span style="color:#6A737D;"># 将上一次迭代的梯度清凉</span></span>
<span class="line"><span style="color:#E1E4E8;">        outputs </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> model(inputs)   </span><span style="color:#6A737D;"># 前向传播</span></span>
<span class="line"><span style="color:#E1E4E8;">        loss </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> criterion(outputs, labels)   </span><span style="color:#6A737D;"># 计算损失</span></span>
<span class="line"><span style="color:#E1E4E8;">        loss.backward()    </span><span style="color:#6A737D;"># 反向传播，获得梯度</span></span>
<span class="line"><span style="color:#E1E4E8;">        optimizer.step()    </span><span style="color:#6A737D;"># 通过梯度，进行参数更新</span></span>
<span class="line"><span style="color:#E1E4E8;">        </span></span>
<span class="line"><span style="color:#6A737D;"># 训练完毕，保存模型</span></span>
<span class="line"><span style="color:#E1E4E8;">torch.save(model, </span><span style="color:#9ECBFF;">&quot;./model.pt&quot;</span><span style="color:#E1E4E8;">)</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#D73A49;">for</span><span style="color:#24292E;"> epoch </span><span style="color:#D73A49;">in</span><span style="color:#24292E;"> </span><span style="color:#005CC5;">range</span><span style="color:#24292E;">(num_epochs):</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#D73A49;">for</span><span style="color:#24292E;"> inputs, labels </span><span style="color:#D73A49;">in</span><span style="color:#24292E;"> train_loader:</span></span>
<span class="line"><span style="color:#24292E;">        </span><span style="color:#6A737D;"># 将tensor移入GPU</span></span>
<span class="line"><span style="color:#24292E;">        inputs </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> inputs.cuda()</span></span>
<span class="line"><span style="color:#24292E;">        labels </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> labels.cuda()</span></span>
<span class="line"><span style="color:#24292E;">        </span></span>
<span class="line"><span style="color:#24292E;">        optimizer.zero_grad()   </span><span style="color:#6A737D;"># 将上一次迭代的梯度清凉</span></span>
<span class="line"><span style="color:#24292E;">        outputs </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> model(inputs)   </span><span style="color:#6A737D;"># 前向传播</span></span>
<span class="line"><span style="color:#24292E;">        loss </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> criterion(outputs, labels)   </span><span style="color:#6A737D;"># 计算损失</span></span>
<span class="line"><span style="color:#24292E;">        loss.backward()    </span><span style="color:#6A737D;"># 反向传播，获得梯度</span></span>
<span class="line"><span style="color:#24292E;">        optimizer.step()    </span><span style="color:#6A737D;"># 通过梯度，进行参数更新</span></span>
<span class="line"><span style="color:#24292E;">        </span></span>
<span class="line"><span style="color:#6A737D;"># 训练完毕，保存模型</span></span>
<span class="line"><span style="color:#24292E;">torch.save(model, </span><span style="color:#032F62;">&quot;./model.pt&quot;</span><span style="color:#24292E;">)</span></span></code></pre></div><h3 id="评估模型" tabindex="-1"><strong>评估模型</strong> <a class="header-anchor" href="#评估模型" aria-label="Permalink to &quot;**评估模型**&quot;">​</a></h3><ul><li>评估模型，我们还需要划分出一个验证/测试集，并且同样建立<code>DataLoader()</code>，假设为<code>val_loader</code></li></ul><div class="language-Python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">Python</span><pre class="shiki github-dark vp-code-dark"><code><span class="line"><span style="color:#6A737D;"># 加载模型</span></span>
<span class="line"><span style="color:#E1E4E8;">model </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> torch.load(</span><span style="color:#9ECBFF;">&quot;./model.pt&quot;</span><span style="color:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#F97583;">for</span><span style="color:#E1E4E8;"> inputs, labels </span><span style="color:#F97583;">in</span><span style="color:#E1E4E8;"> val_loader:</span></span>
<span class="line"><span style="color:#E1E4E8;">    outputs </span><span style="color:#F97583;">=</span><span style="color:#E1E4E8;"> model(inputs)</span></span>
<span class="line"><span style="color:#E1E4E8;">    </span><span style="color:#6A737D;"># 现在的outputs为一个batch的logits，并且shape=(batch_size, 2)</span></span>
<span class="line"><span style="color:#E1E4E8;">    </span><span style="color:#6A737D;"># 请你通过这个outputs矩阵，结合你所挑选的模型评估指标（如准确率），对模型进行评估</span></span>
<span class="line"><span style="color:#E1E4E8;">    </span><span style="color:#6A737D;"># 请自行实现</span></span></code></pre><pre class="shiki github-light vp-code-light"><code><span class="line"><span style="color:#6A737D;"># 加载模型</span></span>
<span class="line"><span style="color:#24292E;">model </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> torch.load(</span><span style="color:#032F62;">&quot;./model.pt&quot;</span><span style="color:#24292E;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#D73A49;">for</span><span style="color:#24292E;"> inputs, labels </span><span style="color:#D73A49;">in</span><span style="color:#24292E;"> val_loader:</span></span>
<span class="line"><span style="color:#24292E;">    outputs </span><span style="color:#D73A49;">=</span><span style="color:#24292E;"> model(inputs)</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#6A737D;"># 现在的outputs为一个batch的logits，并且shape=(batch_size, 2)</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#6A737D;"># 请你通过这个outputs矩阵，结合你所挑选的模型评估指标（如准确率），对模型进行评估</span></span>
<span class="line"><span style="color:#24292E;">    </span><span style="color:#6A737D;"># 请自行实现</span></span></code></pre></div><p><strong>注意</strong>，<strong>上述均只是一个示例，实际在完成本题过程中，需要根据具体任务和数据进行相应的调整和修改。</strong></p><h2 id="思考" tabindex="-1">思考 <a class="header-anchor" href="#思考" aria-label="Permalink to &quot;思考&quot;">​</a></h2><ol><li>你使用的是什么损失函数？请简单介绍这个损失函数，如果有概率论基础并了解最大似然的同学，请尝试推导出损失函数的数学形式。</li><li>在文本分类任务中可能会面临过拟合问题，尤其是当训练数据较少时，可以采用哪些常见的防止过拟合的方法。</li><li>反向传播是一种用于训练神经网络模型的算法，通过计算损失函数对模型参数的梯度，然后利用梯度下降法更新模型参数。请思考并简单推导一下<strong>你所用的这个模型的</strong>反向传播公式。</li><li>在 LSTM 中，输入门、遗忘门和输出门是如何实现其功能的。</li></ol><h2 id="回答要求" tabindex="-1">回答要求 <a class="header-anchor" href="#回答要求" aria-label="Permalink to &quot;回答要求&quot;">​</a></h2><ol><li>处理数据、训练和测试的代码，并大致解释你的代码；</li><li>代码运行结果截图和训练过程中，损失和<strong>评估指标</strong>（<strong>评估指标不限，如准确率</strong>）的变化图像，并在最后使用你所划分的测试集和你选择的评估指标评估模型结果；</li><li>必要的注释说明及良好的代码规范；</li><li>实现思路和学到的知识点。</li></ol><h2 id="本题提交方式" tabindex="-1">本题提交方式 <a class="header-anchor" href="#本题提交方式" aria-label="Permalink to &quot;本题提交方式&quot;">​</a></h2><blockquote><p>收件邮箱：glimmer401@outlook.com</p><p>主题格式：学号-姓名-考核-机器学习-03</p><p>主题示例：2024091202014-张三-考核-机器学习-03</p></blockquote><blockquote><p>出题人QQ：674940575</p><p>出题人邮箱：<a href="mailto:674940575@qq.com" target="_blank" rel="noreferrer">674940575@qq.com</a></p></blockquote>`,52),e=[o];function t(c,r,y,E,i,d){return n(),a("div",null,e)}const h=s(p,[["render",t]]);export{u as __pageData,h as default};
